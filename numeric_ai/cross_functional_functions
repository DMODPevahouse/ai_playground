import random
from typing import List, Dict, Tuple, Callable
import numpy as np



def parse_data(file_name: str) -> List[List]:
    data = []
    file = open(file_name, "r")
    for line in file:
        datum = [value for value in line.rstrip().split(",")]
        data.append(datum)
    random.shuffle(data)
    return data


def create_folds(xs: List, n: int) -> List[List[List]]:
    k, m = divmod(len(xs), n)
    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))



def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:
    training = []
    test = []
    for i, fold in enumerate(folds):
        if i == index:
            test = fold
        else:
            training = training + fold
    return training, test



# <a id="error_rate"></a>
# ## error_rate
#
# This function takes the amount of errors, divided by the total, to determine the percent failed of the model
#
# * **y**: list: list of correct answer
# * **y_hat**: list: list of guesses made by the model
# * **total**: int: the total amount in the set
#
# **returns**: float: returns the percentage, in decimals, of errors found

# In[31]:


def error_rate(y: list, y_hat: list, total: int):
    error_rates = 0
    for i, correct in enumerate(y):
        if y_hat[i] != correct:
            error_rates += 1
    metric = error_rates/total
    return metric


# In[32]:


unit_test = error_rate([1, 2, 3, 1], [1, 1, 1, 1], 4)
assert unit_test == .5
unit_test = error_rate([1, 2, 3, 1], [1, 1, 3, 1], 4)
assert unit_test == .25
unit_test = error_rate([2, 2, 1, 4], [1, 1, 3, 1], 4)
assert unit_test == 1


# <a id="evaluate"></a>
# ## evaluate
#
# This is the function that determines what the actual class labels are, as well as determines what the predicted class labels, and compares them to determine the accuracy of the model **Uses**: [classify](#classify) **Used by**: [cross_validation](#cross_validation)
#
# * **data**: list[list]: This is the data that is being tested against, used for testing a model and training it.
# * **model**: list: This is the model that is being tested.
# * **evaluation_fn**: function: this function is used to actually compare the answer vs what the model got as a class label
# * **classify_fn**: function: this is the function that is used to classify the data given the model and the data
#
# **returns**: float: this function returns the error rate that was achieved by comparing the model to the actuals

# In[33]:


def evaluate(data: list[list], model: list, evaluation_fn, classify_fn):
    y_hats, ys = [], []
    for row in data:
        ys.append(row[0])
    y_hats_results = classify_fn(model, data)
    for row in y_hats_results:
        y_hats.append(row[0])
    metric = evaluation_fn(ys, y_hats, len(y_hats))
    return metric


# In[34]:


unit_test = evaluate(self_check, unit_nbc, error_rate, classify)
assert unit_test == 0.26666666666666666
unit_test = evaluate(self_check[2:], unit_nbc, error_rate, classify)
assert unit_test == 0.3076923076923077
unit_test = evaluate(self_check[4:], unit_nbc, error_rate, classify)
assert unit_test == 0.2727272727272727


# <a id="default_poison"></a>
# ## default_poison
#
# Only use this function has is to just make a list of desired length all to be assigned to the default chosen for testing a baseline
#
# * **length**: int: the length of the string desired to make
# * **default**: str:  the value to set to the list
#
# **returns**: list: returns a list of the default str

# In[35]:


def default_poison(length: int, default: str):
    baseline = []
    for i in list(range(length)):
        baseline.append(default)
    return baseline


# In[36]:


unit_test = default_poison(5, 'a')
assert unit_test == ['a', 'a', 'a', 'a', 'a']
unit_test = default_poison(6, 'h')
assert unit_test == ['h', 'h', 'h', 'h', 'h', 'h']
unit_test = default_poison(6, 'p')
assert unit_test == ['p', 'p', 'p', 'p', 'p', 'p']


# <a id="baseline_cross_validation"></a>
# ## baseline_cross_validation
#
# This function purely sets up a baseline comparison for the 10 fold cross validation that uses the defaul p for every single guess to establish a baseline to test against **Uses**: [error_rate](#error_rate), [evaluate_model](#evaluate_model)
#
# * **folds: List[List[List[float]]]**: here is the set of data that will be tested against, broken into 10 folds
#
#
# **returns**: float: this function returns the error calculation of the test set and model, the ys and y_hats

# In[37]:


def baseline_cross_validation(folds: list[list[list]]):
    train_metrics, test_metrics, n = [], [], len(folds)
    for i in range(0, n):
        train, test = create_train_test(folds, i)
        baseline_train_array, baseline_test_array = default_poison(len(train), 'p'), default_poison(len(test), 'p')
        train_ys, test_ys = [], []
        for row in train:
            train_ys.append(row[0])
        for row in test:
            test_ys.append(row[0])
        metric = error_rate(train_ys, baseline_train_array, len(train))
        train_metrics.append(metric)
        metric = error_rate(test_ys, baseline_test_array, len(test))
        test_metrics.append(metric)
    return train_metrics, test_metrics


# In[38]:


unit_fold = create_folds(self_check, 3)
unit_test = baseline_cross_validation(unit_fold)
assert unit_test == ([1.0, 1.0, 1.0], [1.0, 1.0, 1.0])
unit_fold = create_folds(self_check, 5)
unit_test = baseline_cross_validation(unit_fold)
assert unit_test == ([1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0])
unit_fold = create_folds(self_check, 4)
unit_test = baseline_cross_validation(unit_fold)
assert unit_test == ([1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0])


# <a id="cross_validation"></a>
# ## cross_validation
#
# The purpose of this function is to use 10 crossfold validation by taking in the data as a whole, and also the folds created by create_folds, to have a test set created out of one of the 10 folds that is used to test the other 9, which then cycles down until each of the folds is used to validate the other data **Uses**: [mean_squared_error](#mean_squared), [evaluate_model](#evaluate_model)
#
# * **folds: List[List[List[float]]]**: here is the set of data that will be tested against, broken into 10 folds
# * **model_fn: function: float** This is the baseline model that will be tested against to determine error.
# * **evaluation_fn: function: float**: this is the function that is used to test how the model is doing vs the answers, for here it is mean squared error.
# * **classify_fn**: function: this is the function that is used to classify the data given the model and the data
#
#
# **returns**: float: this function returns the error calculation of the test set and model, the ys and y_hats

# In[39]:


def cross_validation(folds, model_fn, evaluation_fn, classify_fn):
    train_metrics, test_metrics, n = [], [], len(folds)
    for i in range(0, n):
        train, test = create_train_test(folds, i)
        model = model_fn(train)
        metric = evaluate(train, model, evaluation_fn, classify_fn)
        train_metrics.append(metric)
        metric = evaluate(test, model, evaluation_fn, classify_fn)
        test_metrics.append(metric)
    return train_metrics, test_metrics


# In[40]:


unit_train, unit_test = cross_validation(unit_folds, lambda unit_nbc: train(self_check), error_rate, classify)
assert unit_test == [0.0,
 0.3333333333333333,
 0.3333333333333333,
 0.3333333333333333,
 0.3333333333333333]
assert unit_train == [0.3333333333333333, 0.25, 0.25, 0.25, 0.25]
unit_test, unit_train = cross_validation(unit_folds, lambda unit_nbc: train(self_check), error_rate, classify)
assert unit_train == [0.0,
 0.3333333333333333,
 0.3333333333333333,
 0.3333333333333333,
 0.3333333333333333]
assert unit_test == [0.3333333333333333, 0.25, 0.25, 0.25, 0.25]


# <a id="analyzation"></a>
# ## analyzation
#
# This function simply takes in the test and train metrics then lists them out to have an easily read format of what is going on
#
# * **train_metrics: List[List[float]]**: Here is the metrics from running the training set
# * **test_metrics: List[List[float]]**: Here is the metrics from running the testing set
#
#
# **returns**: float: this function returns the error calculation of the test set and model, the ys and y_hats

# In[41]:


def analyzation(train_metrics, test_metrics):
    train_avg, train_std, test_avg, test_std = np.mean(train_metrics), np.std(train_metrics), np.mean(test_metrics), np.std(test_metrics)
    test_avg = np.mean(test_metrics)
    print(f"Fold\tTrain\tTest")
    for i in range(len(train_metrics)):
        train_metrics[i] = train_metrics[i] * 100
        test_metrics[i] = test_metrics[i] * 100
        print(f"{i+1}: \t{train_metrics[i]:.2f}%\t{test_metrics[i]:.2f}%")
    print("\n\n")
    train_avg = train_avg * 100
    test_avg = test_avg * 100
    print(f"avg:\t{train_avg:.2f}%\t{test_avg:.2f}%")
    print(f"std:\t{train_std:.2f}%\t{test_std:.2f}%")